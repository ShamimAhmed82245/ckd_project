# -*- coding: utf-8 -*-
"""ckd-prediction-using-3-classification-algorithms.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FGj-ZoZbRLo6hmIyNFuJXRU5cjUt-7yO

# **Project Title :-**
## **Chronic Kidney Disease Prediction Using Machine Learning Algorithms**

# **Objective**

*   As we know that,Chronic Kidney Disease (CKD) is become major health problem around the world, which results in renal failure, cardiovascular disease, and early death.

*   From research papers, it is found that rate of ckd had been increased up to the 17 % from all over the world.

*   From Reports ,it is found that out of every 10 people 1 people suffering the Chronic Kidney Disease.
*   If Chronic Kidney Disease detected earlier then it can be diagnosed and also
controlled with the help of the medicines.
*   The objective of the project is create model which can determine chances of the chronic kidney disease with the help of the suitable parameters.
*   Also,to find which are the parameters are important to check the chances of the chronic kidney disease.
*   This project will help for early detection of the chronic kidney disease.

# **Data Source**

The dataset taken from the kaggle website which is free to use.The dataset contains 400 rows and 26 columns.The dataset has the 26 features as follows with data types :  


1.   age                             :-  float64
2.   bp[blood pressure]              :-  float64
3.   sg['specific_gravity']          :-  float64
4.   al['albumin']                   :-  float64
5.   su['sugar']                     :-  float64
6.   rbc ['red_blood_cells']         :-  object
7.   pc ['pus_cell',]                :-  object
8.   pcc['pus_cell_clumps']          :-  object
9.   ba['bacteria']                  :-  object
10.  bgr ['blood_glucose_random']    :-  float64
11.  bu['blood_urea']                :-  float64
12.  sc ['serum_creatinine']         :-  float64
13.  sod ['sodium']                  :-  float64
14.  pot  ['potassium']              :-  float64
15.  hemo['haemoglobin']             :-  float64
16.  pcv['packed_cell_volume']       :-  object
17.  wc ['white_blood_cell_count']   :-  object
18.  rc ['red_blood_cell_count']     :-  object
19.  htn ['hypertension']            :-  object
20.  dm  ['diabetes_mellitus']       :-  object
21.  cad ['coronary_artery_disease'] :-  object
22.  appet ['appetite']              :-  object
23.  pe ['peda_edema']               :-  object
24.  ane ['aanemia']                 :-  object
25.  classification                  :-  object


Dataset Link :-https://raw.githubusercontent.com/patilgirish815/Kidney_Cancer_Prediction_Using_Machine_Learning/main/dataset/kidney_disease.csv

# **Import Library**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import sklearn as sk
import seaborn as sns
import missingno as msn
# import plotly.express as px
import pickle
from sklearn.preprocessing import StandardScaler

"""# **Import Data**

## **Reading CSV file from Local Drive**
"""

# df=pd.read_csv(r"kidney_disease.csv")
# df

"""## **Reading CSV file using url file path**"""

df=pd.read_csv(r"https://raw.githubusercontent.com/patilgirish815/Kidney_Cancer_Prediction_Using_Machine_Learning/main/dataset/kidney_disease.csv");

"""# **Describe Data**

### **Shape function:-**
##### The shape function shows the no. of rows and the no. of columns
"""

df.shape

"""### **Head function:-**
##### The shape function shows the 5 records
"""

df.head()

"""### **Tail function:-**
##### The shape function shows the last 5 records
"""

df.tail()

"""### **Describe function:-**
##### The describe function shows the average , minimum ,standard deviation ,maximum,count 25% of column,50% of column,75% of column of the each column.
"""

df.describe(include="all")

"""### **Info function:-**
##### The info function shows no. of null values in the each column and the data type of the each column.
"""

df.info()

"""### **dtypes function:-**
##### The dtypes function shows data type of each column.
"""

df.dtypes

"""# **Data Preprocessing**

### **isnull function:-**
##### The isnull function shows null values present in each column.
"""

df.isnull().sum()

# df.corr()

# plt.figure(figsize=(15,8));
# plt.title("Correlation",color="green")
# sns.heatmap(df.corr(),linewidth=1,annot=True)

"""## **CHECKING NO OF NULL VALUES THROUGH VISUALIZATION**"""

import missingno as msn
msn.bar(df,color="red");

df.isnull()

"""### **duplicated function:-**
##### The duplicated function checks the duplicate values present in the dataset.
"""

df.duplicated().value_counts()

"""## **FINDING COUNT OF CKD AND NOT CKD AND CHANGING CKT/T VALUES TO CKD**"""

df['classification'].value_counts()

df['classification'].unique()

df[df["classification"]=="ckd\t"]

df["classification"]=df["classification"].replace("ckd\t","ckd",regex=True)

plt.figure(figsize=(17,7))
sns.countplot(data=df, x="classification")
plt.title("\nChronic Kidney Disease Distribution\n", fontsize=25)
plt.show();

df["age"].isnull().sum()

df["age"]=df["age"].fillna(df["age"].mean())

df.info()

"""##### *Here,we will fill all values of float64 datatype with the median and mode*"""

numerical=[]
for col in df.columns:
    if df[col].dtype=="float64":
        numerical.append(col)
print(numerical)
for col in df.columns:
    if col in numerical:
        df[col].fillna(df[col].median(), inplace=True)
    else:
        df[col].fillna(df[col].mode()[0], inplace=True)

"""## LABEL ENCODER"""

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
object_col = [col for col in df.columns if df[col].dtype == 'object']
for col in object_col:
    df[col] = le.fit_transform(df[col])

df.info()

df.head()

df.columns

"""# **Data Visualization**

##### *The normal range of the blood urea is between 15 to 40 When blood urea increases above 40 then the chances of the kidney failure increases can be seen  from the scatter plot*
"""

fig, ax = plt.subplots(figsize=(6,7))
M=df[['bu']]
N=df[['classification']]
plt.title("Relation Between Blood Urea And Chronic Kidney Disease",color="red");
plt.xlabel("Blood Urea",color="green")
plt.ylabel("Chronic Kidney Disease(1=Yes,0=No)",color="green")
ax.scatter(M,N);
plt.show();

"""##### *The normal range of the serum creatine is between 0.5 to 1.5 When Serum Creatine increases above 1.5 then the chances of the kidney failure increases can be seen  from the scatter plot*

"""

fig, ax = plt.subplots(figsize=(6,7))
M=df[['sc']]
N=df[['classification']]
plt.title("Relation Between Serum Creatine And Chronic Kidney Disease");
plt.xlabel("Serum Creatine")
plt.ylabel("Chronic Kidney Disease(1=Yes,0=No)")
ax.scatter(M,N);
plt.show();

"""##### *Due to the hypertension mainly kidney failure occurs.*"""

fig, ax = plt.subplots(figsize=(6,7))
M=df[['htn']]
N=df[['classification']]
plt.title("Relation Between Hypertension: yes And Chronic Kidney Disease");
plt.xlabel("Hypertension: yes")
plt.ylabel("Chronic Kidney Disease(1=Yes,0=No)")
ax.scatter(M,N);
plt.show();

"""##### *When urine albumin increses above its normal range then it first leads to protein leakage i.e, the first stage of the kidney failure if not treated or diagonosed at that time which leads to the kidney failure.*"""

fig, ax = plt.subplots(figsize=(6,7))
M=df[['al']]
N=df[['classification']]
plt.title("Relation Between Albumin And Chronic Kidney Disease");
plt.xlabel("Albumin")
plt.ylabel("Chronic Kidney Disease(1=Yes,0=No)")
ax.scatter(M,N);
plt.show();

"""##### *The diabetics is also one of the main reason for the kidney failure.*"""

fig, ax = plt.subplots(figsize=(6,7))
M=df[['dm']]
N=df[['classification']]
plt.title("Relation Between Diabetes Mellitus: yes And Chronic Kidney Disease",color="red");
plt.xlabel("Diabetes Mellitus: yes")
plt.ylabel("Chronic Kidney Disease(1=Yes,0=No)")
ax.scatter(M,N);
plt.show();

sns.boxplot(x=df['classification'], y=df['bu'])
plt.show();

# px.scatter_3d(df, x='age', y='htn', z='su', color='classification')

sns.scatterplot(data=df,x="su",y="htn",hue='classification');

sns.stripplot(x=df["su"]);

sns.catplot(x="htn",y="su",data=df,kind="box");
plt.xlabel("Sugar",color="red")
plt.ylabel("Classification",color="red")
plt.title("Boxplot of Hypertension and Sugar",color="green");

plt.figure(figsize=(20,10))
sns.boxplot(data=df, x="ane", y="hemo", palette='seismic')
plt.xlabel("Hemeoglobin",color="red")
plt.ylabel("Aneamia",color="red")
plt.show()

plt.figure(figsize=(20,10))
sns.boxplot(data=df, y='hemo', x="pcc", hue="ane")
plt.xlabel("Hameoglobin",color="red")
plt.ylabel("Pus Cell Clumps",color="red")
plt.show()

# fig = px.scatter(df_orig,
#     x = df['age'], y = df['dm'],color="class")
# fig.show()







"""### **columns function:-**
##### The columns function column names present in the dataset.
"""

df.columns

"""## **Get Unique Values(Class or Labels) in y variable**

Here,
1=Yes
0=No
"""

df['classification'].shape

df.groupby("classification").mean()

df.columns

"""## **Defining Target or Dependent Variable (y) and Feature or Independent Variables (X)**"""

X=df[[ 'age', 'bp', 'sg', 'al', 'su', 'rbc', 'pc', 'pcc', 'ba', 'bgr',
       'bu', 'sc', 'sod', 'pot', 'hemo', 'pcv', 'wc', 'rc', 'htn', 'dm', 'cad',
       'appet', 'pe', 'ane']]
y=df[['classification']]

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=222)

print("Training Data ::-")
print("The shape of X training data is :-" ,X_train.shape)
print("The shape of y training data is :-" ,y_train.shape)

print("Testing Data ::-")
print("The shape of X testing data is :-" ,X_test.shape)
print("The shape of y testing data is :-" ,y_test.shape)

"""## **Checking the correlated variables using heatmap(Pearson Correlation)**"""

import seaborn as sns
plt.figure(figsize=(20,10))
cor = X_train.corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.CMRmap)
plt.show();

# By using this function we can select correlated features
# it will remove the first feature that is correlated with anything other feature
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns
    corr_matrix = dataset.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr

corr_features = correlation(X_train, 0.75)
len(set(corr_features))

"""Here,"Packed Cell Volume" and "Red Blood Cells (millions/cmm)" are the correlated features so we need to remove them. So,X needs to redefined again."""

corr_features

X_train.drop(corr_features,axis=1)
X_test.drop(corr_features,axis=1)

"""## **Redefining the feature variables**"""

X=df[['age', 'bp', 'sg', 'al', 'su',  'pcc', 'ba', 'bgr','bu', 'sc', 'sod', 'pot', 'hemo', 'pcv', 'wc', 'rc', 'htn', 'dm', 'cad','appet', 'pe', 'ane']]

"""## **Standarlization of X variables**

Standarlization of the dataset is an common requirement for many machine learning algorithms implemented in scikit-learn.they might behave badly if the individual features do not more or less look like standard normally distributed data.Gaussian with zero mean and unit variance.
"""

from sklearn.preprocessing import StandardScaler

sss=StandardScaler()

X=sss.fit_transform(X)

X.shape

# After fitting the scaler, save it
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
pickle.dump(scaler, open('ckd_app/models/scaler.sav', 'wb'))

"""# **Train Test Split**"""

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=222)

print("Training Data ::-")
print("The shape of X training data is :-" ,X_train.shape)
print("The shape of y training data is :-" ,y_train.shape)

print("Testing Data ::-")
print("The shape of X testing data is :-" ,X_test.shape)
print("The shape of y testing data is :-" ,y_test.shape)

"""# **Modeling**

## **1.** ***Logistic Regression***
It is an supervised learning algorithm which is used for the solving classification problems.
"""

# Logistic Regression
from sklearn.linear_model import LogisticRegression
LRmodel=LogisticRegression(max_iter=200,random_state=222)
LRmodel

LRmodel.fit(X_train,y_train)

"""# **Prediction**"""

y_predic=LRmodel.predict(X_test)
print(y_predic)

sns.countplot(y_predic);

"""## **Probability of Each Predicted Class**"""

LRmodel.predict_proba(X_test)

"""# **Model Evaluation**"""

from sklearn.metrics import confusion_matrix, classification_report,accuracy_score

print("Accuracy of the model is :  %3f " % accuracy_score(y_test,y_predic))

print(confusion_matrix(y_test,y_predic))

print(classification_report(y_test, y_predic))

"""# **2.** ***Decision Tree Classifier Algorithm***
Decision Tree is an supervised learning algorithm which is used for both classification and regression problems.Decision tree classifier is mostly used for the classification problems.From decision tree ,get set of rules for classifing the problem.

# **Modeling**
"""

from sklearn.tree import DecisionTreeClassifier
DTmodel=DecisionTreeClassifier(random_state=222)

DTmodel.fit(X_train,y_train)

"""# **Prediction**"""

y_predict=DTmodel.predict(X_test)
print(y_predict)

"""## **Probability of Each Predicted Class**"""

print(DTmodel.predict_proba(X_test))

"""# **Model Evaluation**"""

from sklearn.metrics import confusion_matrix, classification_report,accuracy_score

print(confusion_matrix(y_test,y_predict))

print(classification_report(y_test, y_predict))

"""## **Plotting the Decision Tree**"""

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
plt.figure(figsize=(10,25))
plot_tree(DTmodel,filled=True);

"""# **3.** ***K-Nearest Neighbors***
It is an supervised learning algorithm which is used for both regression and classification problems but mostly used for the classification problems.

# **Modeling**
"""

from sklearn.neighbors import KNeighborsClassifier
KNmodel=KNeighborsClassifier()

KNmodel.fit(X_train,y_train)

"""# **Prediction**"""

y_predict=KNmodel.predict(X_test)
print(y_predict)

"""## **Probability of Each Predicted Class**"""

print(DTmodel.predict_proba(X_test))

"""# **Model Evaluation**"""

from sklearn.metrics import confusion_matrix, classification_report,accuracy_score

print(confusion_matrix(y_test,y_predict))

print(classification_report(y_test, y_predict))

import pickle

# Save the model
filename = 'LR_model.sav'
pickle.dump(LRmodel, open(filename, 'wb'))

# Load the model later
loaded_model = pickle.load(open(filename, 'rb'))

import pickle

# Save the model
filename = 'KN_model.sav'  # Replace with your desired filename
with open(filename, 'wb') as f:
    pickle.dump(KNmodel, f)

# Load the model later
with open(filename, 'rb') as f:
    loaded_model = pickle.load(f)

# prompt: save X_test data so that i can use it for prediction

import pandas as pd
import numpy as np
# ... (rest of your import statements)

# ... (your existing code)

# Save X_test
np.save('X_test_data.npy', X_test)

# prompt: save y_test data so that i can use it for prediction

# Save y_test
np.save('y_test_data.npy', y_test)

# prompt: find the metrics for all model

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Assuming you have already trained the models (LRmodel, DTmodel, KNmodel) and have X_test and y_test

# Function to print metrics for a given model
def print_model_metrics(model, model_name):
    y_pred = model.predict(X_test)
    print(f"\nMetrics for {model_name}:\n")
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
    print("\nClassification Report:\n", classification_report(y_test, y_pred))


# Assuming LRmodel, DTmodel, and KNmodel are already defined and trained
print_model_metrics(LRmodel, "Logistic Regression")
print_model_metrics(DTmodel, "Decision Tree Classifier")
print_model_metrics(KNmodel, "K-Nearest Neighbors")

# prompt: make more ml model and save them

# Assuming you have already trained the models (LRmodel, DTmodel, KNmodel) and have X_test and y_test

# Function to print metrics for a given model
def print_model_metrics(model, model_name):
    y_pred = model.predict(X_test)
    print(f"\nMetrics for {model_name}:\n")
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
    print("\nClassification Report:\n", classification_report(y_test, y_pred))

from sklearn.svm import SVC

# 4. Support Vector Classifier
SVMmodel = SVC(random_state=222)
SVMmodel.fit(X_train, y_train)
print_model_metrics(SVMmodel, "Support Vector Classifier")

# Save the SVM model
filename = 'SVM_model.sav'
pickle.dump(SVMmodel, open(filename, 'wb'))

from sklearn.ensemble import RandomForestClassifier

# 5. RandomForestClassifier
RFmodel = RandomForestClassifier(random_state=222)
RFmodel.fit(X_train,y_train)
print_model_metrics(RFmodel, "Random Forest Classifier")

# Save the RandomForestClassifier model
filename = 'RF_model.sav'
pickle.dump(RFmodel, open(filename, 'wb'))

"""# **Explanation**
In this project,For classification of the chronic kidney disease three algorithms are used which are


1.   Logistic Regression
2.   Decision Tree Classifier
3.   K-Nearest Neighbour

the model has got the accuracies as 99%,100% and 98% respectively. from these accuracies it is found that model performs well for decision tree classifier.The accuracy of  other models can be increased by changing train-test split ratio and taking significant features.


"""